{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P6SAaSKg569"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "# 1. Data Loading and Preprocessing\n",
        "# Load the dataset\n",
        "df = pd.read_csv('customer.csv', sep='\\t')\n",
        "\n",
        "# Handle Missing Values (Income has ~24 missing values)\n",
        "df['Income'] = df['Income'].fillna(df['Income'].median())\n",
        "\n",
        "# Feature Engineering: Convert Date to Tenure (Days)\n",
        "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y')\n",
        "max_date = df['Dt_Customer'].max()\n",
        "df['Customer_Days'] = (max_date - df['Dt_Customer']).dt.days\n",
        "\n",
        "# Drop non-predictive columns\n",
        "# ID is an identifier, Dt_Customer is replaced by Customer_Days\n",
        "# Z_CostContact and Z_Revenue are constant values in this specific dataset\n",
        "df = df.drop(columns=['ID', 'Dt_Customer', 'Z_CostContact', 'Z_Revenue'])\n",
        "\n",
        "# Identify Categorical Features\n",
        "# CatBoost handles these natively without One-Hot Encoding\n",
        "categorical_features_indices = ['Education', 'Marital_Status']\n",
        "\n",
        "# Split Data\n",
        "X = df.drop(columns=['Response'])\n",
        "y = df['Response']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Fitting/Training\n",
        "# Initialize CatBoostClassifier\n",
        "# We use standard Gradient Boosting parameters.\n",
        "# To mimic Random Forest behavior, one would typically use 'Plain' boosting\n",
        "# with subsampling, but the default Boosting is generally superior.\n",
        "model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    loss_function='Logloss',\n",
        "    verbose=100,  # Print every 100 iterations\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "# Train the model, passing categorical features explicitly\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    cat_features=categorical_features_indices,\n",
        "    eval_set=(X_test, y_test),\n",
        "    plot=False\n",
        ")\n",
        "\n",
        "# 3. Evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# 4. Visualization (Code for creating plots)\n",
        "# Feature Importance\n",
        "feature_importance = model.get_feature_importance()\n",
        "sorted_idx = np.argsort(feature_importance)[::-1]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(X.shape[1]), feature_importance[sorted_idx], align='center')\n",
        "plt.xticks(range(X.shape[1]), np.array(X.columns)[sorted_idx], rotation=90)\n",
        "plt.title('CatBoost Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}